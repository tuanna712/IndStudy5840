{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a233ec",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6fa2dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "limit = 60\n",
    "# Load MNIST dataset\n",
    "train_dataset = MNIST(root='../data', train=True, download=False, transform=transforms.ToTensor())\n",
    "test_dataset  = MNIST(root='../data', train=False, download=False, transform=transforms.ToTensor())\n",
    "train_dataset = Subset(train_dataset, range(limit))\n",
    "test_dataset  = Subset(test_dataset, range(limit))\n",
    "\n",
    "# Data loaders\n",
    "mini_batch_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "mini_batch_test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =    0   loss = 138.3624\n",
      "iteration =    1   loss = 139.6690\n",
      "iteration =    2   loss = 139.6690\n",
      "iteration =    3   loss = 139.6690\n",
      "iteration =    4   loss = 139.6690\n",
      "iteration =    5   loss = 139.6690\n",
      "iteration =    6   loss = 139.6690\n",
      "iteration =    7   loss = 139.6690\n",
      "iteration =    8   loss = 139.6690\n",
      "iteration =    9   loss = 139.6690\n",
      "iteration =   10   loss = 139.6690\n",
      "iteration =   11   loss = 139.6690\n",
      "iteration =   12   loss = 139.6690\n",
      "iteration =   13   loss = 139.6690\n",
      "iteration =   14   loss = 139.6690\n",
      "iteration =   15   loss = 139.6690\n",
      "iteration =   16   loss = 139.6690\n",
      "iteration =   17   loss = 139.6690\n",
      "iteration =   18   loss = 139.6690\n",
      "iteration =   19   loss = 139.6690\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(28 * 28, 10)\n",
    "        nn.init.uniform_(self.linear.weight, -0.01, 0.01) \n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True).values)\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def cross_entropy_loss(outputs, labels):\n",
    "    log_probs = torch.log(outputs)\n",
    "    loss = -torch.sum(labels * log_probs, dim=1)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "max_iter = 20\n",
    "model = LogisticRegression()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.LBFGS(model.parameters(), max_iter=max_iter)\n",
    "\n",
    "model.train()\n",
    "for itr in range(max_iter):\n",
    "    itr_loss = 0.0\n",
    "    for i, (images, label) in enumerate(train_dataset):\n",
    "        # print(f\"Batch {i+1}/{len(train_dataset)}\")    \n",
    "        def loss_closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss_val = criterion(outputs, torch.tensor([label]))\n",
    "            loss_val.backward()\n",
    "            return loss_val\n",
    "        \n",
    "        optimizer.step(loss_closure)\n",
    "\n",
    "        # outputs = model(images)\n",
    "        loss = loss_closure()\n",
    "        itr_loss += loss.item()\n",
    "\n",
    "        # print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"iteration = %4d   loss = %0.4f\" % (itr, itr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b96775f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Name: linear.weight\n",
      "Parameter Shape: torch.Size([10, 784])\n",
      "Parameter Values: tensor([[-0.0223,  0.0014, -0.0142,  ..., -0.0314,  0.0112, -0.0295],\n",
      "        [ 0.0312,  0.0293,  0.0177,  ..., -0.0296, -0.0038, -0.0329],\n",
      "        [-0.0283, -0.0137, -0.0010,  ...,  0.0102,  0.0261,  0.0173],\n",
      "        ...,\n",
      "        [ 0.0088,  0.0012,  0.0073,  ..., -0.0333,  0.0355, -0.0108],\n",
      "        [ 0.0077, -0.0244,  0.0250,  ...,  0.0328, -0.0055, -0.0100],\n",
      "        [ 0.0144, -0.0184,  0.0133,  ..., -0.0283, -0.0104, -0.0076]])\n",
      "Parameter Name: linear.bias\n",
      "Parameter Shape: torch.Size([10])\n",
      "Parameter Values: tensor([-0.0173,  0.0190, -0.0232, -0.0220,  0.0068, -0.0030,  0.0239,  0.0156,\n",
      "         0.0302,  0.0242])\n"
     ]
    }
   ],
   "source": [
    "# Print out all information in (model.parameters())\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter Name: {name}\")\n",
    "    print(f\"Parameter Shape: {param.shape}\")\n",
    "    print(f\"Parameter Values: {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba9428a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[ 2.8763,  0.0279, -0.4859,  0.3026,  0.8348],\n",
      "        [ 1.2505, -0.1971, -0.6957,  1.0116, -1.1143],\n",
      "        [-0.8433, -1.8453,  0.2537, -0.2584,  0.7560]], requires_grad=True)\n",
      "Target: tensor([2, 1, 2])\n",
      "Output Loss: 2.398954153060913\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(f\"Input: {input}\")\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(f\"Target: {target}\")\n",
    "output = loss(input, target)\n",
    "print(f\"Output Loss: {output.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "84543c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[ 0.4588,  1.6342,  0.2501, -1.0978, -0.7803],\n",
      "        [ 0.0698, -0.6374,  1.7318, -0.1972, -0.9900],\n",
      "        [-1.5758, -1.8658,  2.0320, -0.4963, -0.4279]], requires_grad=True)\n",
      "Target: tensor([[0.3987, 0.1619, 0.0975, 0.3067, 0.0351],\n",
      "        [0.0325, 0.0933, 0.4083, 0.2549, 0.2110],\n",
      "        [0.7586, 0.0757, 0.1208, 0.0358, 0.0092]])\n",
      "Output Loss: 2.3818016052246094\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(f\"Input: {input}\")\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(f\"Target: {target}\")\n",
    "output = loss(input, target)\n",
    "print(f\"Output Loss: {output.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74939a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
