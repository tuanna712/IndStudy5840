{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch CNN - CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full dataset\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('.ipynb').resolve().parents[2]))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "DOWNLOAD = False\n",
    "SUBSET = 0\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomAffine(degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='/home/tung5534/cnn_cifar/data', \n",
    "                                            train=True,\n",
    "                                            download=DOWNLOAD,\n",
    "                                            transform=train_transform,\n",
    "                                            )\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='/home/tung5534/cnn_cifar/data', \n",
    "                                            train=False,\n",
    "                                            download=DOWNLOAD,\n",
    "                                            transform=test_transform,\n",
    "                                            )\n",
    "if SUBSET != 0:\n",
    "    subset_indices = list(range(SUBSET))\n",
    "    train_set = torch.utils.data.Subset(train_dataset, subset_indices)\n",
    "    test_set = torch.utils.data.Subset(test_dataset, subset_indices)\n",
    "    print(f'Using subset of {SUBSET} samples')\n",
    "else:\n",
    "    train_set, test_set = train_dataset, test_dataset\n",
    "    print('Using full dataset')\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Class names\n",
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_performance(save_path,\n",
    "                     train_losses, test_losses, train_errs, \n",
    "                     test_errs, train_accs, test_acc, run_times,\n",
    "                     n_step):\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    if n_step < 1:\n",
    "        lr = str(n_step).replace(\".\",\"\")\n",
    "        \n",
    "    performance = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'train_errs': train_errs,\n",
    "        'test_errs': test_errs,\n",
    "        'train_accs': train_accs,\n",
    "        'test_acc': test_acc,\n",
    "        'run_time': run_times,\n",
    "        'n_step': n_step\n",
    "    }\n",
    "    with open(f'{save_path}/FISTA_v2_lr_{n_step}.json', 'w') as f:\n",
    "        json.dump(performance, f, indent=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, test_loader=test_loader):\n",
    "    model.eval() \n",
    "    _test_acc, _test_err, _test_loss, total_test = 0, 0, 0, 0\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                _test_acc += (predicted == labels).sum().item()\n",
    "                _test_err += (predicted != labels).sum().item()\n",
    "                _test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    test_loss = _test_loss / total_test\n",
    "    test_err = 100 * _test_err / total_test\n",
    "    test_acc = 100 * _test_acc / total_test\n",
    "\n",
    "    return test_loss, test_err, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.optim.optimizer import Optimizer\n",
    "\n",
    "# class FISTA(Optimizer):\n",
    "#     def __init__(self, params, lr=1e-3, ministeps = 5):\n",
    "#         defaults = dict(lr=lr, ministeps=ministeps)\n",
    "#         super(FISTA, self).__init__(params, defaults)\n",
    "\n",
    "#         for group in self.param_groups:\n",
    "#             group['y'] = [p.clone().detach() for p in group['params']]\n",
    "#             group['k'] = 1\n",
    "\n",
    "#     def step(self, closure=None):\n",
    "#         if closure is None:\n",
    "#             raise RuntimeError(\"FISTA requires a closure to recompute gradients.\")\n",
    "        \n",
    "#         loss = None\n",
    "#         for group in self.param_groups:\n",
    "#             lr, ministeps = group['lr'], group['ministeps']\n",
    "\n",
    "#             # Loop over ministeps\n",
    "#             for _ in range(ministeps):\n",
    "# #                 k = group['k']\n",
    "#                 k = 1\n",
    "\n",
    "#                 with torch.enable_grad():\n",
    "#                     loss = closure()\n",
    "\n",
    "#                 # Reset, k=1, update p\n",
    "#                 for i, p in enumerate(group['params']):\n",
    "#                     if p.grad is None:\n",
    "#                         continue\n",
    "\n",
    "#                     x_k = p.data\n",
    "#                     y_k = group['y'][i].data\n",
    "\n",
    "#                     # y_k grad\n",
    "#                     grad_y = p.grad.data\n",
    "\n",
    "#                     # x_{k+1} = y_k - t * grad(f(y_k))\n",
    "#                     x_next = y_k - lr * grad_y\n",
    "#                     # Question: Should we update multi-step here, inside the loop\n",
    "\n",
    "#                     #  y_{k+1} = x_{k+1} + (k / (k + 3)) * (x_{k+1} - x_k)\n",
    "#                     momentum_coeff = (k) / (k + 3)\n",
    "#                     y_next = x_next + momentum_coeff * (x_next - x_k)\n",
    "\n",
    "#                     # Update\n",
    "#                     p.data = x_next\n",
    "#                     group['y'][i].data = y_next\n",
    "            \n",
    "# #             group['k'] += 1\n",
    "\n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from models import SimpleCNN\n",
    "from optim.fista_multi_wo_prox import FISTA\n",
    "\n",
    "def modeling(n_epochs=100, lr=0.01, threshold=90):\n",
    "    model = SimpleCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = FISTA(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses, test_losses, train_errs, test_errs, train_accs, test_accs, run_times = [], [], [], [], [], [], []\n",
    "\n",
    "    for i, epoch in enumerate(range(n_epochs)):\n",
    "        print(f\"Epoch: {i+1}/{n_epochs}\")\n",
    "        model.train() \n",
    "        total_train, _train_err, _train_acc, running_loss, run_time = 0, 0, 0, 0.0, 0\n",
    "        _start = time.time()\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            # Backward\n",
    "            loss = optimizer.step(closure)\n",
    "\n",
    "            # Log Losses and Training accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            _train_err += (predicted != labels).sum().item()\n",
    "            _train_acc += (predicted == labels).sum().item()\n",
    "\n",
    "        run_time = time.time() - _start\n",
    "        \n",
    "        epoch_train_loss = running_loss / total_train\n",
    "        epoch_train_acc = 100 * _train_acc / total_train\n",
    "        epoch_train_err = 100 * _train_err / total_train\n",
    "        \n",
    "        test_loss, test_err, test_acc = evaluate_model(model, criterion, test_loader)\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_errs.append(epoch_train_err)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_errs.append(test_err)\n",
    "        test_accs.append(test_acc)\n",
    "        run_times.append(run_time)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'E [{epoch+1}/{n_epochs}]. train_loss_acc: {running_loss / len(train_loader):.4f}, {epoch_train_acc:.2f}%, '\n",
    "                    f'test_acc: {test_acc:.2f}%, run_time: {round(run_time, 2)} seconds')\n",
    "        if epoch_train_acc >= threshold:\n",
    "            print(f\"Early stopping at epoch {epoch+1} with train error {epoch_train_err:.2f}%\")\n",
    "            break\n",
    "    return train_losses, test_losses, train_errs, test_errs, train_accs, test_accs, run_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/29\n",
      "E [1/29]. train_loss_acc: 0.1876, 52.56%, test_acc: 63.60%, run_time: 10.733123302459717\n",
      "Epoch: 2/29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90\u001b[39m\n\u001b[1;32m      6\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m train_losses, test_losses, train_errs, test_errs, train_accs, test_accs, run_times \u001b[38;5;241m=\u001b[39m \u001b[43mmodeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                                                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_scores\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m save_performance(save_path,\n\u001b[1;32m     14\u001b[0m                  train_losses, test_losses, train_errs, \n\u001b[1;32m     15\u001b[0m                  test_errs, train_accs, test_accs, run_times, n_step\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     16\u001b[0m                  )\n",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m, in \u001b[0;36mmodeling\u001b[0;34m(n_epochs, lr, threshold)\u001b[0m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Log Losses and Training accuracy\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m total_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from plot import metrics_plot\n",
    "\n",
    "n_epochs = 29\n",
    "threshold = 90\n",
    "lr = 0.1\n",
    "\n",
    "train_losses, test_losses, train_errs, test_errs, train_accs, test_accs, run_times = modeling(n_epochs=n_epochs,\n",
    "                                                                                          lr=lr,\n",
    "                                                                                          threshold=threshold,\n",
    "                                                                                         )\n",
    "save_path = f'epoch_scores'\n",
    "save_performance(save_path,\n",
    "                 train_losses, test_losses, train_errs, \n",
    "                 test_errs, train_accs, test_accs, run_times, n_step=lr,\n",
    "                 )\n",
    "actual_nepochs = len(train_losses)\n",
    "metrics_plot(actual_nepochs, train_losses, test_losses, train_accs, test_accs, train_errs, test_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch24",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
