{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch CNN - CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import os, sys\n",
    "\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('.ipynb').resolve().parents[2]))\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def mini_batching(BATCH_SIZE, DOWNLOAD = False, SUBSET = 0):\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.RandomHorizontalFlip(p=0.5),\n",
    "         transforms.RandomAffine(degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # Load dataset\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='/home/tung5534/cnn_cifar/data', \n",
    "                                                train=True,\n",
    "                                                download=DOWNLOAD,\n",
    "                                                transform=train_transform,\n",
    "                                                )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='/home/tung5534/cnn_cifar/data', \n",
    "                                                train=False,\n",
    "                                                download=DOWNLOAD,\n",
    "                                                transform=test_transform,\n",
    "                                                )\n",
    "    if SUBSET != 0:\n",
    "        subset_indices = list(range(SUBSET))\n",
    "        train_set = torch.utils.data.Subset(train_dataset, subset_indices)\n",
    "        test_set = torch.utils.data.Subset(test_dataset, subset_indices)\n",
    "        print(f\"Using a subset of {SUBSET} samples for training and testing.\")\n",
    "    else:\n",
    "        train_set, test_set = train_dataset, test_dataset\n",
    "        print(\"Using the full dataset for training and testing.\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_performance(save_path, method,\n",
    "                     train_losses, test_losses, train_errs, \n",
    "                     test_errs, train_accs, test_acc, run_times,\n",
    "                     n_step, lr, batch_size):\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    lr = str(lr).replace('.', '')\n",
    "\n",
    "    performance = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'train_errs': train_errs,\n",
    "        'test_errs': test_errs,\n",
    "        'train_accs': train_accs,\n",
    "        'test_acc': test_acc,\n",
    "        'run_time': run_times,\n",
    "        'n_step': n_step,\n",
    "        'lr':lr,\n",
    "        'batch_size':batch_size,\n",
    "    }\n",
    "    with open(f'{save_path}/{method}_{n_step}_{lr}_{batch_size}.json', 'w') as f:\n",
    "        json.dump(performance, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, test_loader):\n",
    "    model.eval() \n",
    "    _test_acc, _test_err, _test_loss, total_test = 0, 0, 0, 0\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                _test_acc += (predicted == labels).sum().item()\n",
    "                _test_err += (predicted != labels).sum().item()\n",
    "                _test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    test_loss = _test_loss / total_test\n",
    "    test_err = 100 * _test_err / total_test\n",
    "    test_acc = 100 * _test_acc / total_test\n",
    "\n",
    "    return test_loss, test_err, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "def viz_scores(scores_path, threshold):\n",
    "    contents = os.walk(scores_path)\n",
    "    lr_runtime, lr_epochs, runtimes, epochs, lrs = {}, {}, [], [], []\n",
    "    train_loss_dict = {}\n",
    "    train_acc_dict = {}\n",
    "\n",
    "    for root, dirs, files in contents:\n",
    "        for f in files:\n",
    "            if f.endswith(\"json\"):\n",
    "                _path = os.path.join(root, f)\n",
    "                with open(_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                lrs.append(float(data['n_step'][0] + '.' + data['n_step'][1:]))\n",
    "                runtimes.append(sum(data['run_time']))\n",
    "                epochs.append(len(data['run_time']))\n",
    "\n",
    "                train_loss_dict[data['n_step']] = data['train_losses']\n",
    "                train_acc_dict[data['n_step']] = data['train_accs']\n",
    "    lr_runtime['lr'] = lrs\n",
    "    lr_runtime['run_time'] = runtimes\n",
    "    lr_epochs['lr'] = lrs\n",
    "    lr_epochs['nepochs'] = epochs\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(lr_runtime['lr'], lr_runtime['run_time'], label='Runtime', marker='o',linestyle='', color='blue', alpha=.5)\n",
    "    ax2.plot(lr_epochs['lr'], lr_epochs['nepochs'], label='Epochs', marker='o', linestyle='', color='red', alpha=.5)\n",
    "\n",
    "    ax1.set_ylabel('Runtime (s)')\n",
    "    ax2.set_ylabel('Epochs')\n",
    "\n",
    "    ax1.set_xlabel('Stepsize')\n",
    "\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='best')\n",
    "\n",
    "    plt.title(f'Step size evaluation to get {threshold}% train acc')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return train_loss_dict, train_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tung5534/cnn_cifar'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "str(Path('.ipynb').resolve().parents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path('.ipynb').resolve().parents[2]))\n",
    "\n",
    "from models import SimpleCNN\n",
    "from optim.sgd_sngl import OneStepSGD\n",
    "from optim.sgd_mult_v2 import ManyStepSGD\n",
    "import time \n",
    "\n",
    "def modeling(train_loader, test_loader, n_step=2, n_epochs=15, lr=0.133, momentum=0, threshold=90):\n",
    "    model = SimpleCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if n_step == 1:\n",
    "        optimizer = OneStepSGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = ManyStepSGD(model.parameters(), lr=lr, momentum=momentum, n_step=n_step)\n",
    "\n",
    "    train_losses, test_losses, train_errs, test_errs, train_accs, test_accs, run_times = [], [], [], [], [], [], []\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    for i, epoch in enumerate(range(n_epochs)):\n",
    "#         print(f\"Epoch: {i+1}/{n_epochs}\")\n",
    "        model.train() \n",
    "        total_train, _train_err, _train_acc, running_loss, run_time = 0, 0, 0, 0.0, 0\n",
    "        _start = time.time()\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if n_step == 1:\n",
    "                # Forward\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                # Forward\n",
    "                outputs = model(images)\n",
    "                # Backward\n",
    "                loss = optimizer.step(closure)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # Training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            _train_err += (predicted != labels).sum().item()\n",
    "            _train_acc += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_train_loss = running_loss / total_train\n",
    "        epoch_train_acc = 100 * _train_acc / total_train\n",
    "        epoch_train_err = 100 * _train_err / total_train\n",
    "        run_time = time.time() - _start\n",
    "\n",
    "        test_loss, test_err, test_acc = evaluate_model(model, criterion, test_loader)\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_errs.append(epoch_train_err)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_errs.append(test_err)\n",
    "        test_accs.append(test_acc)\n",
    "        run_times.append(run_time)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(f'E [{epoch+1}/{n_epochs}]. train_loss_acc: {running_loss / len(train_loader):.4f}, {epoch_train_acc:.2f}%, '\n",
    "                    f'test_acc: {test_acc:.2f}%, run_time: {run_time}')\n",
    "        if epoch_train_acc >= threshold:\n",
    "            print(f\"Early stopping at epoch {epoch+1} with train acc {epoch_train_acc:.2f}%\")\n",
    "            break\n",
    "    return train_losses, test_losses, train_errs, test_errs, train_accs, test_accs, run_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the full dataset for training and testing.\n",
      "E [1/30]. train_loss_acc: 0.0826, 48.13%, test_acc: 61.54%, run_time: 13.919124841690063\n",
      "E [2/30]. train_loss_acc: 0.0114, 62.51%, test_acc: 67.68%, run_time: 13.920006275177002\n",
      "E [3/30]. train_loss_acc: 0.0092, 68.31%, test_acc: 71.11%, run_time: 13.922752141952515\n",
      "E [4/30]. train_loss_acc: 0.0082, 71.39%, test_acc: 73.35%, run_time: 13.916241645812988\n",
      "E [5/30]. train_loss_acc: 0.0075, 74.07%, test_acc: 74.70%, run_time: 13.931274890899658\n",
      "E [6/30]. train_loss_acc: 0.0070, 75.88%, test_acc: 76.68%, run_time: 13.943676233291626\n",
      "E [7/30]. train_loss_acc: 0.0063, 77.50%, test_acc: 76.17%, run_time: 13.931981086730957\n",
      "E [8/30]. train_loss_acc: 0.0059, 78.78%, test_acc: 79.08%, run_time: 13.915026426315308\n",
      "E [9/30]. train_loss_acc: 0.0055, 79.96%, test_acc: 78.23%, run_time: 13.91064977645874\n",
      "E [10/30]. train_loss_acc: 0.0051, 80.94%, test_acc: 78.80%, run_time: 13.939643383026123\n",
      "E [11/30]. train_loss_acc: 0.0049, 82.09%, test_acc: 79.80%, run_time: 13.924765348434448\n",
      "E [12/30]. train_loss_acc: 0.0046, 82.73%, test_acc: 79.79%, run_time: 13.940415382385254\n",
      "E [13/30]. train_loss_acc: 0.0044, 83.39%, test_acc: 80.26%, run_time: 13.90057635307312\n",
      "E [14/30]. train_loss_acc: 0.0042, 84.11%, test_acc: 80.74%, run_time: 13.91231632232666\n",
      "E [15/30]. train_loss_acc: 0.0039, 84.74%, test_acc: 80.11%, run_time: 13.92212963104248\n",
      "E [16/30]. train_loss_acc: 0.0037, 85.19%, test_acc: 81.11%, run_time: 13.944250106811523\n",
      "Early stopping at epoch 16 with train acc 85.19%\n",
      "Using the full dataset for training and testing.\n",
      "E [1/30]. train_loss_acc: 0.1873, 48.54%, test_acc: 60.06%, run_time: 16.816694259643555\n",
      "E [2/30]. train_loss_acc: 0.0151, 63.14%, test_acc: 67.65%, run_time: 16.766947031021118\n",
      "E [3/30]. train_loss_acc: 0.0112, 69.45%, test_acc: 70.30%, run_time: 16.783737659454346\n",
      "E [4/30]. train_loss_acc: 0.0093, 72.29%, test_acc: 72.08%, run_time: 16.77867889404297\n",
      "E [5/30]. train_loss_acc: 0.0082, 74.88%, test_acc: 73.50%, run_time: 16.770103216171265\n",
      "E [6/30]. train_loss_acc: 0.0072, 76.72%, test_acc: 74.96%, run_time: 16.785640954971313\n",
      "E [7/30]. train_loss_acc: 0.0066, 78.44%, test_acc: 75.37%, run_time: 16.77714443206787\n",
      "E [8/30]. train_loss_acc: 0.0061, 79.73%, test_acc: 76.40%, run_time: 16.789941787719727\n",
      "E [9/30]. train_loss_acc: 0.0058, 80.67%, test_acc: 77.38%, run_time: 16.795447826385498\n",
      "E [10/30]. train_loss_acc: 0.0053, 81.83%, test_acc: 77.77%, run_time: 16.776658058166504\n",
      "E [11/30]. train_loss_acc: 0.0048, 82.93%, test_acc: 79.00%, run_time: 16.792640447616577\n",
      "E [12/30]. train_loss_acc: 0.0046, 83.50%, test_acc: 78.87%, run_time: 16.778937578201294\n",
      "E [13/30]. train_loss_acc: 0.0043, 84.40%, test_acc: 79.40%, run_time: 16.780160665512085\n",
      "E [14/30]. train_loss_acc: 0.0040, 85.06%, test_acc: 79.11%, run_time: 16.79141402244568\n",
      "Early stopping at epoch 14 with train acc 85.06%\n",
      "Using the full dataset for training and testing.\n",
      "E [1/30]. train_loss_acc: 0.5498, 48.64%, test_acc: 63.49%, run_time: 20.757319688796997\n",
      "E [2/30]. train_loss_acc: 0.0320, 63.95%, test_acc: 68.59%, run_time: 20.810037851333618\n",
      "E [3/30]. train_loss_acc: 0.0181, 69.44%, test_acc: 71.90%, run_time: 20.81901788711548\n",
      "E [4/30]. train_loss_acc: 0.0144, 72.68%, test_acc: 74.47%, run_time: 20.78931951522827\n",
      "E [5/30]. train_loss_acc: 0.0112, 75.33%, test_acc: 75.46%, run_time: 20.792661666870117\n",
      "E [6/30]. train_loss_acc: 0.0097, 76.91%, test_acc: 76.02%, run_time: 20.771255016326904\n",
      "E [7/30]. train_loss_acc: 0.0086, 78.41%, test_acc: 77.51%, run_time: 20.791399478912354\n",
      "E [8/30]. train_loss_acc: 0.0077, 80.01%, test_acc: 77.99%, run_time: 20.777165412902832\n",
      "E [9/30]. train_loss_acc: 0.0069, 80.64%, test_acc: 78.63%, run_time: 20.91999578475952\n",
      "E [10/30]. train_loss_acc: 0.0064, 82.01%, test_acc: 79.72%, run_time: 20.765331268310547\n",
      "E [11/30]. train_loss_acc: 0.0060, 83.02%, test_acc: 78.72%, run_time: 20.765255451202393\n",
      "E [12/30]. train_loss_acc: 0.0055, 83.91%, test_acc: 79.89%, run_time: 20.76481580734253\n",
      "E [13/30]. train_loss_acc: 0.0052, 84.55%, test_acc: 79.71%, run_time: 20.75930142402649\n",
      "E [14/30]. train_loss_acc: 0.0048, 85.36%, test_acc: 79.85%, run_time: 20.75551199913025\n",
      "Early stopping at epoch 14 with train acc 85.36%\n",
      "Using the full dataset for training and testing.\n",
      "E [1/30]. train_loss_acc: 1.2664, 43.42%, test_acc: 58.61%, run_time: 21.88065481185913\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m BATCH_SIZES:\n\u001b[1;32m      8\u001b[0m     train_loader, test_loader \u001b[38;5;241m=\u001b[39m mini_batching(batch_size)\n\u001b[0;32m----> 9\u001b[0m     train_losses, test_losses, train_errs, test_errs, train_accs, test_accs, run_times \u001b[38;5;241m=\u001b[39m \u001b[43mmodeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                                                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores/E30T85\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[52], line 53\u001b[0m, in \u001b[0;36mmodeling\u001b[0;34m(train_loader, test_loader, n_step, n_epochs, lr, momentum, threshold)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     loss \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[0;32m---> 53\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Training accuracy\u001b[39;00m\n\u001b[1;32m     55\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from plot import metrics_plot\n",
    "n_epochs = 30\n",
    "n_step = 10\n",
    "lr = 0.133\n",
    "threshold = 85\n",
    "BATCH_SIZES = [128, 256, 512, #1024, 2048, 4096]\n",
    "for batch_size in BATCH_SIZES:\n",
    "    train_loader, test_loader = mini_batching(batch_size)\n",
    "    train_losses, test_losses, train_errs, test_errs, train_accs, test_accs, run_times = modeling(n_step=n_step, \n",
    "                                                                                               n_epochs=n_epochs, \n",
    "                                                                                               lr=lr,\n",
    "                                                                                               threshold=threshold,\n",
    "                                                                                               train_loader=train_loader,\n",
    "                                                                                               test_loader=test_loader,\n",
    "                                                                                      )\n",
    "    save_path = 'scores/E30T85'\n",
    "    method = 'SGD'\n",
    "    save_performance(save_path, method,\n",
    "                     train_losses, test_losses, train_errs, \n",
    "                     test_errs, train_accs, test_accs, run_times, n_step, lr, batch_size\n",
    "                     )\n",
    "#     actual_nepochs = len(train_losses)\n",
    "#     metrics_plot(actual_nepochs, train_losses, test_losses, train_accs, test_accs, train_errs, test_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_path_1 = \"scores/E20T60\"\n",
    "train_loss_dict_1, train_acc_dict_1 = viz_scores(scores_path_1, threshold=60)\n",
    "scores_path_2 = \"scores/E30T90\"\n",
    "train_loss_dict_2, train_acc_dict_2 = viz_scores(scores_path_2, threshold=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tung5534/cnn_cifar/W08/optimal_batch_size'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Single vs Multi Descent Steps SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sngl_sgd = \"scores/E30T90/SGD_lr_0133_sngl.json\"\n",
    "mult_05_sgd = \"scores/E30T90/SGD_lr_0133_mult_05.json\"\n",
    "mult_03_sgd = \"scores/E30T90/SGD_lr_0133_mult_03.json\"\n",
    "mult_10_sgd = \"scores/E30T90/SGD_lr_0133_mult_10.json\"\n",
    "\n",
    "with open(sngl_sgd, 'r') as f:\n",
    "    sngl_scores = json.load(f)\n",
    "with open(mult_03_sgd, 'r') as f:\n",
    "    mult_03_scores = json.load(f)\n",
    "with open(mult_05_sgd, 'r') as f:\n",
    "    mult_05_scores = json.load(f)\n",
    "with open(mult_10_sgd, 'r') as f:\n",
    "    mult_10_scores = json.load(f)\n",
    "\n",
    "print(sngl_scores.keys())\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, len(sngl_scores['train_errs'][:30])+1), sngl_scores['train_errs'][:30], linestyle='-', label='Single SGD')\n",
    "plt.plot(range(1, len(mult_03_scores['train_errs'])+1), mult_03_scores['train_errs'], linestyle='-', label='Multi (03) SGD')\n",
    "plt.plot(range(1, len(mult_05_scores['train_errs'])+1), mult_05_scores['train_errs'], linestyle='-', label='Multi (05) SGD')\n",
    "plt.plot(range(1, len(mult_10_scores['train_errs'])+1), mult_10_scores['train_errs'], linestyle='-', label='Multi (10) SGD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Error Rate (%)')\n",
    "plt.title('Performance Comparison of Single vs Multi - Descent Steps SGD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch24",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
